{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs/group/jin/asun/baselines/0_datasets\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/gpfs/home/asun/jin_lab/baselines/0_datasets\")\n",
    "print(os.getcwd())\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "import lightning as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/home/asun/miniforge3/envs/baselines/lib/python3.13/importlib/__init__.py:129: UserWarning: The NumPy module was reloaded (imported a second time). This can in some cases result in small but subtle issues and is discouraged.\n",
      "  _bootstrap._exec(spec, module)\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(np)\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Numba needs NumPy 2.2 or less. Got NumPy 2.3.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscanpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msc\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/baselines/lib/python3.13/site-packages/scanpy/__init__.py:9\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpackaging\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Version\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m check_versions\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_version\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[32m     12\u001b[39m check_versions()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/baselines/lib/python3.13/site-packages/scanpy/_utils/__init__.py:40\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_compat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CSBase, DaskArray, _CSMatrix, _register_union\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_settings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m settings\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompute\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mis_constant\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_constant  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m Version(anndata_version) >= Version(\u001b[33m\"\u001b[39m\u001b[33m0.10.0\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01manndata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse_dataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     44\u001b[39m         BaseCompressedSparseDataset \u001b[38;5;28;01mas\u001b[39;00m SparseDataset,\n\u001b[32m     45\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/baselines/lib/python3.13/site-packages/scanpy/_utils/compute/is_constant.py:7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumbers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Integral\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, overload\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumba\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_compat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CSCBase, CSRBase, DaskArray, _register_union, njit\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/baselines/lib/python3.13/site-packages/numba/__init__.py:59\u001b[39m\n\u001b[32m     54\u001b[39m             msg = (\u001b[33m\"\u001b[39m\u001b[33mNumba requires SciPy version 1.0 or greater. Got SciPy \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     55\u001b[39m                    \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscipy.__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     56\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[43m_ensure_critical_deps\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# END DO NOT MOVE\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# ---------------------- WARNING WARNING WARNING ----------------------------\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_version\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_versions\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/baselines/lib/python3.13/site-packages/numba/__init__.py:45\u001b[39m, in \u001b[36m_ensure_critical_deps\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m numpy_version > (\u001b[32m2\u001b[39m, \u001b[32m2\u001b[39m):\n\u001b[32m     43\u001b[39m     msg = (\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNumba needs NumPy 2.2 or less. Got NumPy \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     44\u001b[39m            \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumpy_version[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumpy_version[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: Numba needs NumPy 2.2 or less. Got NumPy 2.3."
     ]
    }
   ],
   "source": [
    "import scanpy as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import lightning as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Sequential(nn.Linear(28 * 28, 64), nn.ReLU(), nn.Linear(64, 3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.l1(x)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Sequential(nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, 28 * 28))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.l1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitAutoEncoder(L.LightningModule):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        x, _ = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        loss = F.mse_loss(x_hat, x)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MNIST(os.getcwd(), download=True, transform=transforms.ToTensor())\n",
    "train_loader = DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "autoencoder = LitAutoEncoder(Encoder(), Decoder())\n",
    "\n",
    "# train model\n",
    "trainer = L.Trainer()\n",
    "trainer.fit(model=autoencoder, train_dataloaders=train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PertDataModule(L.LightningDataModule):\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_train, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import lightning as L\n",
    "import numpy as np\n",
    "import anndata as ad\n",
    "\n",
    "# -----------------------------\n",
    "# PyTorch Dataset wrapper\n",
    "# -----------------------------\n",
    "class AnnDataset(Dataset):\n",
    "    def __init__(self, adata, idx, obs_cols=None, target_layer=\"X\"):\n",
    "        \"\"\"\n",
    "        adata: AnnData\n",
    "        idx: indices (np.ndarray) for this split\n",
    "        obs_cols: list of obs columns to include as covariates (optional)\n",
    "        target_layer: which layer/matrix to use as y (default .X)\n",
    "        \"\"\"\n",
    "        self.adata = adata\n",
    "        self.idx = np.array(idx)\n",
    "        self.obs_cols = obs_cols or []\n",
    "        self.target_layer = target_layer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        i = self.idx[i]\n",
    "        # Targets: expression matrix\n",
    "        if self.target_layer == \"X\":\n",
    "            y = self.adata.X[i].toarray().squeeze() if hasattr(self.adata.X, \"toarray\") else self.adata.X[i]\n",
    "        else:\n",
    "            y = self.adata.layers[self.target_layer][i]\n",
    "\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "        # Optional covariates (categorical or numeric from obs)\n",
    "        if self.obs_cols:\n",
    "            obs_vals = [self.adata.obs[col].iloc[i] for col in self.obs_cols]\n",
    "            x = torch.tensor(np.array(obs_vals), dtype=torch.float32)\n",
    "        else:\n",
    "            x = torch.tensor([])\n",
    "\n",
    "        return x, y\n",
    "\n",
    "# -----------------------------\n",
    "# LightningDataModule\n",
    "# -----------------------------\n",
    "class AnnDataModule(L.LightningDataModule):\n",
    "    def __init__(self, adata, batch_size=256, obs_cols=None, target_layer=\"X\",\n",
    "                 train_idx=None, val_idx=None, test_idx=None):\n",
    "        super().__init__()\n",
    "        self.adata = adata\n",
    "        self.batch_size = batch_size\n",
    "        self.obs_cols = obs_cols\n",
    "        self.target_layer = target_layer\n",
    "        self.train_idx = train_idx\n",
    "        self.val_idx = val_idx\n",
    "        self.test_idx = test_idx\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Default random split if none provided\n",
    "        n = self.adata.n_obs\n",
    "        if self.train_idx is None:\n",
    "            idx = np.arange(n)\n",
    "            np.random.shuffle(idx)\n",
    "            n_train = int(0.8*n); n_val = int(0.1*n)\n",
    "            self.train_idx, self.val_idx, self.test_idx = idx[:n_train], idx[n_train:n_train+n_val], idx[n_train+n_val:]\n",
    "\n",
    "        self.ds_train = AnnDataset(self.adata, self.train_idx, self.obs_cols, self.target_layer)\n",
    "        self.ds_val   = AnnDataset(self.adata, self.val_idx,   self.obs_cols, self.target_layer)\n",
    "        self.ds_test  = AnnDataset(self.adata, self.test_idx,  self.obs_cols, self.target_layer)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.ds_train, batch_size=self.batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.ds_val, batch_size=self.batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.ds_test, batch_size=self.batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boli = sc.read_h5ad('/gpfs/home/asun/jin_lab/baselines/0_datasets/boli_ctx.h5ad')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baselines",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
