{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-02 13:59:39.921703: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/gpfs/home/asun/miniforge3/envs/perturbench/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/attr_value.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/gpfs/home/asun/miniforge3/envs/perturbench/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/gpfs/home/asun/miniforge3/envs/perturbench/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/resource_handle.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/gpfs/home/asun/miniforge3/envs/perturbench/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_shape.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/gpfs/home/asun/miniforge3/envs/perturbench/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/types.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/gpfs/home/asun/miniforge3/envs/perturbench/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/full_type.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/gpfs/home/asun/miniforge3/envs/perturbench/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/function.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/gpfs/home/asun/miniforge3/envs/perturbench/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/node_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/gpfs/home/asun/miniforge3/envs/perturbench/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/op_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/gpfs/home/asun/miniforge3/envs/perturbench/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/gpfs/home/asun/miniforge3/envs/perturbench/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph_debug_info.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/gpfs/home/asun/miniforge3/envs/perturbench/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/versions.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/gpfs/home/asun/miniforge3/envs/perturbench/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/gpfs/home/asun/miniforge3/envs/perturbench/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at xla/tsl/protobuf/coordination_config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/gpfs/home/asun/miniforge3/envs/perturbench/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/cost_graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/gpfs/home/asun/miniforge3/envs/perturbench/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/step_stats.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/gpfs/home/asun/miniforge3/envs/perturbench/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/allocation_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/gpfs/home/asun/miniforge3/envs/perturbench/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/gpfs/home/asun/miniforge3/envs/perturbench/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/cluster.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/gpfs/home/asun/miniforge3/envs/perturbench/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/debug.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No TensorBoard event files found in /gpfs/home/asun/jin_lab/perturbench/debug/out_L6_CT_CTX_holdout/version_0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 128\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m# Usage example\u001b[39;00m\n\u001b[32m    127\u001b[39m log_dir = \u001b[33m\"\u001b[39m\u001b[33m/gpfs/home/asun/jin_lab/perturbench/debug/out_L6_CT_CTX_holdout/version_0\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m metrics_df, saturation_metrics = \u001b[43mextract_all_metrics\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_csv\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtraining_metrics.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    131\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[38;5;66;03m# Access specific saturation point\u001b[39;00m\n\u001b[32m    134\u001b[39m \u001b[38;5;66;03m#train_saturation_step = saturation_metrics['train/loss']['saturation_step']\u001b[39;00m\n\u001b[32m    135\u001b[39m \u001b[38;5;66;03m#print(f\"\\nTraining speed saturation: {train_saturation_step} steps\")\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 102\u001b[39m, in \u001b[36mextract_all_metrics\u001b[39m\u001b[34m(log_dir, output_csv)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[33;03mExtract all metrics and calculate saturation for train/val loss.\u001b[39;00m\n\u001b[32m     92\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     99\u001b[39m \u001b[33;03m- saturation_metrics: Dictionary with saturation analysis\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m# Read metrics\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m metrics_df = \u001b[43mread_tensorboard_logs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[38;5;66;03m# Save to CSV if requested\u001b[39;00m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_csv:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mread_tensorboard_logs\u001b[39m\u001b[34m(log_dir)\u001b[39m\n\u001b[32m     18\u001b[39m event_files = \u001b[38;5;28mlist\u001b[39m(log_path.glob(\u001b[33m\"\u001b[39m\u001b[33mevents.out.tfevents.*\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m event_files:\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo TensorBoard event files found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlog_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m all_metrics = []\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m event_file \u001b[38;5;129;01min\u001b[39;00m event_files:\n",
      "\u001b[31mValueError\u001b[39m: No TensorBoard event files found in /gpfs/home/asun/jin_lab/perturbench/debug/out_L6_CT_CTX_holdout/version_0"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.core.util import event_pb2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def read_tensorboard_logs(log_dir):\n",
    "    \"\"\"\n",
    "    Read TensorBoard event files and extract metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    - log_dir: Path to directory containing TensorBoard event files\n",
    "    \n",
    "    Returns:\n",
    "    - metrics_df: DataFrame with columns [step, epoch, metric_name, value]\n",
    "    \"\"\"\n",
    "    log_path = Path(log_dir)\n",
    "    event_files = list(log_path.glob(\"events.out.tfevents.*\"))\n",
    "    \n",
    "    if not event_files:\n",
    "        raise ValueError(f\"No TensorBoard event files found in {log_dir}\")\n",
    "    \n",
    "    all_metrics = []\n",
    "    \n",
    "    for event_file in event_files:\n",
    "        for record in tf.data.TFRecordDataset(str(event_file)):\n",
    "            event = event_pb2.Event.FromString(record.numpy())\n",
    "            \n",
    "            # Extract scalar values\n",
    "            for value in event.summary.value:\n",
    "                all_metrics.append({\n",
    "                    'step': event.step,\n",
    "                    'wall_time': event.wall_time,\n",
    "                    'metric_name': value.tag,\n",
    "                    'value': value.simple_value\n",
    "                })\n",
    "    \n",
    "    metrics_df = pd.DataFrame(all_metrics)\n",
    "    return metrics_df\n",
    "\n",
    "def calculate_saturation_point(metrics_df, metric_name='train/loss', threshold_pct=0.1):\n",
    "    \"\"\"\n",
    "    Calculate the first step where the model reaches within threshold_pct of final loss.\n",
    "    \n",
    "    Parameters:\n",
    "    - metrics_df: DataFrame from read_tensorboard_logs\n",
    "    - metric_name: Name of the metric to analyze (e.g., 'train/loss')\n",
    "    - threshold_pct: Percentage threshold (default 0.1 for 10%)\n",
    "    \n",
    "    Returns:\n",
    "    - saturation_dict: Dictionary with saturation metrics\n",
    "    \"\"\"\n",
    "    # Filter for the specific metric\n",
    "    metric_data = metrics_df[metrics_df['metric_name'] == metric_name].copy()\n",
    "    metric_data = metric_data.sort_values('step')\n",
    "    \n",
    "    if len(metric_data) == 0:\n",
    "        raise ValueError(f\"Metric '{metric_name}' not found in data\")\n",
    "    \n",
    "    # Get final loss value (average of last 10% of training)\n",
    "    n_final = max(1, len(metric_data) // 10)\n",
    "    final_loss = metric_data['value'].tail(n_final).mean()\n",
    "    \n",
    "    # Get initial loss\n",
    "    initial_loss = metric_data['value'].iloc[0]\n",
    "    \n",
    "    # Calculate threshold value\n",
    "    threshold_value = final_loss * (1 + threshold_pct)\n",
    "    \n",
    "    # Find first step where loss is within threshold of final value\n",
    "    within_threshold = metric_data[metric_data['value'] <= threshold_value]\n",
    "    \n",
    "    if len(within_threshold) == 0:\n",
    "        saturation_step = None\n",
    "        saturation_epoch = None\n",
    "    else:\n",
    "        saturation_step = within_threshold['step'].iloc[0]\n",
    "        saturation_epoch = saturation_step  # Adjust if you track epochs separately\n",
    "    \n",
    "    return {\n",
    "        'saturation_step': saturation_step,\n",
    "        'initial_loss': initial_loss,\n",
    "        'final_loss': final_loss,\n",
    "        'threshold_value': threshold_value,\n",
    "        'total_steps': metric_data['step'].max(),\n",
    "        'saturation_pct': (saturation_step / metric_data['step'].max() * 100) if saturation_step else None\n",
    "    }\n",
    "\n",
    "def extract_all_metrics(log_dir, output_csv=None):\n",
    "    \"\"\"\n",
    "    Extract all metrics and calculate saturation for train/val loss.\n",
    "    \n",
    "    Parameters:\n",
    "    - log_dir: Path to TensorBoard log directory\n",
    "    - output_csv: Optional path to save metrics CSV\n",
    "    \n",
    "    Returns:\n",
    "    - metrics_df: DataFrame with all metrics\n",
    "    - saturation_metrics: Dictionary with saturation analysis\n",
    "    \"\"\"\n",
    "    # Read metrics\n",
    "    metrics_df = read_tensorboard_logs(log_dir)\n",
    "    \n",
    "    # Save to CSV if requested\n",
    "    if output_csv:\n",
    "        metrics_df.to_csv(output_csv, index=False)\n",
    "        print(f\"Metrics saved to {output_csv}\")\n",
    "    \n",
    "    # Calculate saturation for different metrics\n",
    "    saturation_metrics = {}\n",
    "    \n",
    "    for metric_name in ['train_loss', 'val_loss_step']:\n",
    "        try:\n",
    "            saturation = calculate_saturation_point(metrics_df, metric_name)\n",
    "            saturation_metrics[metric_name] = saturation\n",
    "            print(f\"\\n{metric_name}:\")\n",
    "            print(f\"  Saturation step: {saturation['saturation_step']}\")\n",
    "            print(f\"  Saturation at: {saturation['saturation_pct']:.1f}% of training\")\n",
    "            print(f\"  Initial loss: {saturation['initial_loss']:.4f}\")\n",
    "            print(f\"  Final loss: {saturation['final_loss']:.4f}\")\n",
    "        except ValueError:\n",
    "            continue\n",
    "    \n",
    "    return metrics_df, saturation_metrics\n",
    "\n",
    "# Usage example\n",
    "log_dir = \"/gpfs/home/asun/jin_lab/perturbench/debug/out_L6_CT_CTX_holdout/version_0\"\n",
    "metrics_df, saturation_metrics = extract_all_metrics(\n",
    "    log_dir, \n",
    "    output_csv=\"training_metrics.csv\"\n",
    ")\n",
    "\n",
    "# Access specific saturation point\n",
    "#train_saturation_step = saturation_metrics['train/loss']['saturation_step']\n",
    "#print(f\"\\nTraining speed saturation: {train_saturation_step} steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metric_name\n",
       "val_loss_step                   560\n",
       "epoch                           278\n",
       "train_loss                      268\n",
       "lr-Adam                          10\n",
       "lr-Adam-momentum                 10\n",
       "val_loss_epoch                   10\n",
       "hp_metric                         2\n",
       "_hparams_/experiment              2\n",
       "_hparams_/session_start_info      2\n",
       "_hparams_/session_end_info        2\n",
       "rmse_average                      1\n",
       "rmse_rank_average                 1\n",
       "cosine_logfc                      1\n",
       "cosine_rank_logfc                 1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df[\"metric_name\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available metrics: ['batch_time_avg', 'batch_time_cv_percent', 'batch_time_max', 'batch_time_max_min_ratio', 'batch_time_min', 'batches_per_second', 'decoder_loss', 'epoch', 'step', 'train_loss', 'val/decoder_loss', 'val_loss']\n",
      "Total rows: 1200\n",
      "\n",
      "decoder_loss:\n",
      "  Saturation step: 649\n",
      "  Saturation at: 1.6% of training\n",
      "  Initial loss: 8.2925\n",
      "  Final loss: 1.5523\n",
      "\n",
      "train_loss:\n",
      "  Saturation step: 1499\n",
      "  Saturation at: 3.7% of training\n",
      "  Initial loss: 8.1391\n",
      "  Final loss: 0.5946\n",
      "\n",
      "val/decoder_loss:\n",
      "  Saturation step: 5999\n",
      "  Saturation at: 15.0% of training\n",
      "  Initial loss: 4.2559\n",
      "  Final loss: 0.8933\n",
      "\n",
      "val_loss:\n",
      "  Saturation step: 7399\n",
      "  Saturation at: 18.5% of training\n",
      "  Initial loss: 2.4406\n",
      "  Final loss: 0.2517\n",
      "\n",
      "Saturation summary saved to saturation_summary.json\n",
      "\n",
      "Training speed saturation: 1499 steps\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def read_csv_metrics(csv_path):\n",
    "    \"\"\"\n",
    "    Read metrics from CSV file (e.g., from wandb logger).\n",
    "    \n",
    "    Parameters:\n",
    "    - csv_path: Path to CSV file with metrics\n",
    "    \n",
    "    Returns:\n",
    "    - metrics_df: DataFrame with metrics\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    return df\n",
    "\n",
    "def calculate_saturation_point(metrics_df, metric_name='train_loss', threshold_pct=0.1):\n",
    "    \"\"\"\n",
    "    Calculate the first step where the model reaches within threshold_pct of final loss.\n",
    "    \n",
    "    Parameters:\n",
    "    - metrics_df: DataFrame with columns including 'step' and the metric column\n",
    "    - metric_name: Name of the metric column to analyze (e.g., 'train_loss')\n",
    "    - threshold_pct: Percentage threshold (default 0.1 for 10%)\n",
    "    \n",
    "    Returns:\n",
    "    - saturation_dict: Dictionary with saturation metrics\n",
    "    \"\"\"\n",
    "    # Filter for rows where the metric exists (not NaN)\n",
    "    metric_data = metrics_df[['step', metric_name]].dropna().copy()\n",
    "    metric_data = metric_data.sort_values('step')\n",
    "    \n",
    "    if len(metric_data) == 0:\n",
    "        raise ValueError(f\"Metric '{metric_name}' not found or has no valid data\")\n",
    "    \n",
    "    # Get final loss value (average of last 10% of training)\n",
    "    n_final = max(1, len(metric_data) // 10)\n",
    "    final_loss = metric_data[metric_name].tail(n_final).mean()\n",
    "    \n",
    "    # Get initial loss\n",
    "    initial_loss = metric_data[metric_name].iloc[0]\n",
    "    \n",
    "    # Calculate threshold value\n",
    "    threshold_value = final_loss * (1 + threshold_pct)\n",
    "    \n",
    "    # Find first step where loss is within threshold of final value\n",
    "    within_threshold = metric_data[metric_data[metric_name] <= threshold_value]\n",
    "    \n",
    "    if len(within_threshold) == 0:\n",
    "        saturation_step = None\n",
    "        saturation_pct = None\n",
    "    else:\n",
    "        saturation_step = int(within_threshold['step'].iloc[0])\n",
    "        total_steps = int(metric_data['step'].max())\n",
    "        saturation_pct = (saturation_step / total_steps * 100)\n",
    "    \n",
    "    return {\n",
    "        'saturation_step': saturation_step,\n",
    "        'initial_loss': float(initial_loss),\n",
    "        'final_loss': float(final_loss),\n",
    "        'threshold_value': float(threshold_value),\n",
    "        'total_steps': int(metric_data['step'].max()),\n",
    "        'saturation_pct': saturation_pct\n",
    "    }\n",
    "\n",
    "def extract_all_metrics(csv_path, output_summary=None):\n",
    "    \"\"\"\n",
    "    Extract all metrics and calculate saturation for train/val loss.\n",
    "    \n",
    "    Parameters:\n",
    "    - csv_path: Path to CSV file with metrics\n",
    "    - output_summary: Optional path to save saturation summary JSON\n",
    "    \n",
    "    Returns:\n",
    "    - metrics_df: DataFrame with all metrics\n",
    "    - saturation_metrics: Dictionary with saturation analysis\n",
    "    \"\"\"\n",
    "    # Read metrics\n",
    "    metrics_df = read_csv_metrics(csv_path)\n",
    "    \n",
    "    print(f\"Available metrics: {list(metrics_df.columns)}\")\n",
    "    print(f\"Total rows: {len(metrics_df)}\")\n",
    "    \n",
    "    # Calculate saturation for different metrics\n",
    "    saturation_metrics = {}\n",
    "    \n",
    "    # Look for loss metrics in the columns\n",
    "    loss_columns = [col for col in metrics_df.columns if 'loss' in col.lower() and col != 'step']\n",
    "    \n",
    "    for metric_name in loss_columns:\n",
    "        try:\n",
    "            saturation = calculate_saturation_point(metrics_df, metric_name)\n",
    "            saturation_metrics[metric_name] = saturation\n",
    "            print(f\"\\n{metric_name}:\")\n",
    "            print(f\"  Saturation step: {saturation['saturation_step']}\")\n",
    "            if saturation['saturation_pct']:\n",
    "                print(f\"  Saturation at: {saturation['saturation_pct']:.1f}% of training\")\n",
    "            print(f\"  Initial loss: {saturation['initial_loss']:.4f}\")\n",
    "            print(f\"  Final loss: {saturation['final_loss']:.4f}\")\n",
    "        except (ValueError, KeyError) as e:\n",
    "            print(f\"  Could not calculate saturation for {metric_name}: {e}\")\n",
    "    \n",
    "    # Save saturation summary if requested\n",
    "    if output_summary:\n",
    "        import json\n",
    "        with open(output_summary, 'w') as f:\n",
    "            json.dump(saturation_metrics, f, indent=2)\n",
    "        print(f\"\\nSaturation summary saved to {output_summary}\")\n",
    "    \n",
    "    return metrics_df, saturation_metrics\n",
    "\n",
    "# Usage example\n",
    "csv_path = \"/gpfs/home/asun/jin_lab/perturbench/debug/out_L6_CT_CTX_holdout/version_0/metrics.csv\"\n",
    "metrics_df, saturation_metrics = extract_all_metrics(\n",
    "    csv_path, \n",
    "    output_summary=\"saturation_summary.json\"\n",
    ")\n",
    "\n",
    "# Access specific saturation point\n",
    "if 'train_loss' in saturation_metrics:\n",
    "    train_saturation_step = saturation_metrics['train_loss']['saturation_step']\n",
    "    print(f\"\\nTraining speed saturation: {train_saturation_step} steps\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "perturbbench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
